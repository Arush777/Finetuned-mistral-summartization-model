{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1570a8bda4f6419a9ed07f61aed0392a":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":[],"layout":"IPY_MODEL_35567e4e7b7e47169355c149c540bffd"}},"ed6e4a4ee2de469e82036ba90e5241aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a39402aa990f447a8bb1fccca7a1dced","placeholder":"​","style":"IPY_MODEL_d81a85b99d1d4f1e932d44b17038e854","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"3c7724d8b9a64f229844297dcac06544":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_e7e9bfa76f49449c9f0f1df163c96f0f","placeholder":"​","style":"IPY_MODEL_25de459c9db74d1abb76fdab20d76f86","value":""}},"853b398f153f4224aa53fdb8288070f6":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_1f01d7e404d84550a4c0f024487de376","style":"IPY_MODEL_8dcaccb573844ab6a9fb6ba4afd17e23","value":true}},"d703dc453fe6452890dd464abfc63889":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_df324516f1624fc7aed25501d669fe2a","style":"IPY_MODEL_75f547bd83f64ceea794becbdfdaef73","tooltip":""}},"006074bcc2da47af81356fb496bdbd3c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3acdb5fa0564c9bb810dbd2c6aa9050","placeholder":"​","style":"IPY_MODEL_4e5135a22bf04c1da9f1dc09caabf634","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"35567e4e7b7e47169355c149c540bffd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"a39402aa990f447a8bb1fccca7a1dced":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d81a85b99d1d4f1e932d44b17038e854":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7e9bfa76f49449c9f0f1df163c96f0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25de459c9db74d1abb76fdab20d76f86":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f01d7e404d84550a4c0f024487de376":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dcaccb573844ab6a9fb6ba4afd17e23":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df324516f1624fc7aed25501d669fe2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75f547bd83f64ceea794becbdfdaef73":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"c3acdb5fa0564c9bb810dbd2c6aa9050":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e5135a22bf04c1da9f1dc09caabf634":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a268f09147a492aab7dbc2e693c8952":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_219b4a6d1882413ebe81a2dde7e1d4e9","placeholder":"​","style":"IPY_MODEL_fbc4aac7f8e84ef8930f174abcf271a5","value":"Connecting..."}},"219b4a6d1882413ebe81a2dde7e1d4e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbc4aac7f8e84ef8930f174abcf271a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e083b6bf2bea43e4b6619af8d599ecf8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1b5bee9d2cd646fa90c9782b1f0f63dd","IPY_MODEL_16ccde6770f44757930585a6526c3047","IPY_MODEL_0db12825b4d84778ad8c1f5783ff54db"],"layout":"IPY_MODEL_592c044366f248f7ac8944fb4e9c918e"}},"1b5bee9d2cd646fa90c9782b1f0f63dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c069cf4e5e34f648e9dafe3fa95e27e","placeholder":"​","style":"IPY_MODEL_fca95be2c0484bc890af1133be0cd35c","value":"Loading checkpoint shards: 100%"}},"16ccde6770f44757930585a6526c3047":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_428103eb39964cb4a787f7a67c72ef37","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_31ef1faeaa824b5c8794f3846877dc12","value":2}},"0db12825b4d84778ad8c1f5783ff54db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52aec0de4b724a2ab82d21b078e664ec","placeholder":"​","style":"IPY_MODEL_212f0fdba3f2427996fe6cd605c6fd77","value":" 2/2 [01:07&lt;00:00, 31.60s/it]"}},"592c044366f248f7ac8944fb4e9c918e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c069cf4e5e34f648e9dafe3fa95e27e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fca95be2c0484bc890af1133be0cd35c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"428103eb39964cb4a787f7a67c72ef37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31ef1faeaa824b5c8794f3846877dc12":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"52aec0de4b724a2ab82d21b078e664ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"212f0fdba3f2427996fe6cd605c6fd77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install accelerate transformers peft bitsandbytes datasets evaluate bert-score rouge-score\n","metadata":{"id":"C1HXWM2e3Fco","outputId":"294dc342-bc86-4f4e-d21f-071d6f1785dc","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T16:02:14.950337Z","iopub.execute_input":"2025-03-20T16:02:14.950636Z","iopub.status.idle":"2025-03-20T16:02:24.365585Z","shell.execute_reply.started":"2025-03-20T16:02:14.950614Z","shell.execute_reply":"2025-03-20T16:02:24.364771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset, Dataset\nimport torch","metadata":{"id":"J7U79Gv13IkN","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T16:02:24.366613Z","iopub.execute_input":"2025-03-20T16:02:24.366839Z","iopub.status.idle":"2025-03-20T16:02:29.315453Z","shell.execute_reply.started":"2025-03-20T16:02:24.36682Z","shell.execute_reply":"2025-03-20T16:02:29.314561Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install trl","metadata":{"id":"Pr0GfxOq3e3y","outputId":"6646c448-077d-4fe4-bc95-28e1dcdfb026","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T16:02:29.317196Z","iopub.execute_input":"2025-03-20T16:02:29.317795Z","iopub.status.idle":"2025-03-20T16:02:33.138238Z","shell.execute_reply.started":"2025-03-20T16:02:29.317769Z","shell.execute_reply":"2025-03-20T16:02:33.137339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig,DataCollatorForLanguageModeling\nfrom peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model,PeftModel\nfrom trl import SFTConfig, SFTTrainer, setup_chat_format\nimport torch\nimport os","metadata":{"id":"wRbk3q2u3Pn6","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T16:02:33.139889Z","iopub.execute_input":"2025-03-20T16:02:33.140252Z","iopub.status.idle":"2025-03-20T16:02:52.226093Z","shell.execute_reply.started":"2025-03-20T16:02:33.140217Z","shell.execute_reply":"2025-03-20T16:02:52.225463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd","metadata":{"id":"qkzGDtn93mMU","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T16:02:52.226742Z","iopub.execute_input":"2025-03-20T16:02:52.226945Z","iopub.status.idle":"2025-03-20T16:02:52.230412Z","shell.execute_reply.started":"2025-03-20T16:02:52.226928Z","shell.execute_reply":"2025-03-20T16:02:52.229591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n\n\ndata_df = pd.DataFrame(dataset)\n","metadata":{"id":"E_TN5ZJR6ERM","outputId":"e9d695bb-cd2a-4cef-c7b1-0a3583a85ade","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T16:02:52.231383Z","iopub.execute_input":"2025-03-20T16:02:52.231705Z","iopub.status.idle":"2025-03-20T16:03:21.696316Z","shell.execute_reply.started":"2025-03-20T16:02:52.231675Z","shell.execute_reply":"2025-03-20T16:03:21.695402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_df = data_df.drop(columns=[\"id\"])\ndata_df","metadata":{"id":"Gwk3gxekB-1i","outputId":"d0c0a0ad-eada-40c8-c497-800d1c90899e","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T16:03:21.69707Z","iopub.execute_input":"2025-03-20T16:03:21.697314Z","iopub.status.idle":"2025-03-20T16:03:21.754074Z","shell.execute_reply.started":"2025-03-20T16:03:21.697294Z","shell.execute_reply":"2025-03-20T16:03:21.75338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T16:03:21.756148Z","iopub.execute_input":"2025-03-20T16:03:21.756372Z","iopub.status.idle":"2025-03-20T16:03:26.586929Z","shell.execute_reply.started":"2025-03-20T16:03:21.756353Z","shell.execute_reply":"2025-03-20T16:03:26.586245Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_df[\"prompt\"] = data_df.apply(\n    lambda x: \"Summarize the following article:\\n\" + x[\"article\"] + \"\\n\\nSummary:\\n\" + x[\"highlights\"],\n    axis=1\n)\n","metadata":{"id":"S21ihAa_CAsr","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T16:03:26.588072Z","iopub.execute_input":"2025-03-20T16:03:26.588334Z","iopub.status.idle":"2025-03-20T16:03:30.28804Z","shell.execute_reply.started":"2025-03-20T16:03:26.588312Z","shell.execute_reply":"2025-03-20T16:03:30.28734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_df.head(5)","metadata":{"id":"brzDP7AZDo-Z","outputId":"20f632d2-80f6-4a19-91fb-129e8a8c2ed2","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T16:03:30.288774Z","iopub.execute_input":"2025-03-20T16:03:30.28899Z","iopub.status.idle":"2025-03-20T16:03:30.296929Z","shell.execute_reply.started":"2025-03-20T16:03:30.288957Z","shell.execute_reply":"2025-03-20T16:03:30.296116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SFTFineTuner:\n  def __init__(self,model_name,data_df,output_dir):\n    \"\"\"initalization of class parameter\"\"\"\n    print(\"Parameters Intializaed\")\n    self.model_name=model_name\n    self.data_df=data_df\n    self.output_dir=output_dir\n    self.tokenizer=None\n    self.model=None\n    self.tokenized_data=None\n\n\n  def load_tokenizer(self):\n    \"defining tokenizer of the model\"\n    self.tokenizer=AutoTokenizer.from_pretrained(self.model_name,trust_remote_code=True)\n    self.tokenizer.pad_token=self.tokenizer.eos_token\n    self.tokenizer.padding_side = \"left\"  \n\n\n\n\n  def load_model(self):\n    \"defining the model\"\n\n    ##Quantization\n    bnb_config=BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16\n    )\n\n    self.model=AutoModelForCausalLM.from_pretrained(\n        self.model_name,\n        device_map={\"\":0},\n        trust_remote_code=True,\n        quantization_config=bnb_config\n    )\n\n    # Prepare the model for efficient k-bit (4-bit) training (QLoRA)\n    self.model=prepare_model_for_kbit_training(self.model)\n\n\n\n  def apply_lora(self):\n    \"applying lora to the model\"\n    config=LoraConfig(\n        r=8,\n        lora_alpha=16,\n        target_modules=[\"q_proj\",\"v_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n\n    #applied lora on quatized model\n    self.model=get_peft_model(self.model,config)\n    self.model.print_trainable_parameters()\n\n  def tokenize_dataset(self):\n    \"performing tokenization on the data\"\n    dataset = Dataset.from_pandas(self.data_df)\n\n    # Tokenize the data\n    def tokenize(sample):\n        return self.tokenizer(sample[\"prompt\"], padding=True, truncation=True, max_length=512)\n\n    self.tokenized_data = dataset.map(tokenize, batched=True, desc=\"Tokenizing data\", remove_columns=dataset.column_names)\n\n\n\n  def train_model(self, epochs: int = 1, batch_size: int = 8, learning_rate: float = 2e-4, max_steps: int = 700):\n      args = SFTConfig(\n          output_dir=self.output_dir,               # Directory to save model checkpoints\n          num_train_epochs=epochs,                  # Number of training epochs\n          per_device_train_batch_size=batch_size,   # Batch size per GPU\n          gradient_accumulation_steps=2,            # Accumulate gradients for larger effective batch\n          gradient_checkpointing=True,              # Trade compute for memory savings\n          optim=\"adamw_torch_fused\",                # Use fused AdamW for efficiency\n          learning_rate=learning_rate,              # Learning rate (QLoRA paper)\n          max_grad_norm=0.3,                        # Gradient clipping threshold\n          lr_scheduler_type=\"cosine_with_restarts\", # Faster convergence\n          warmup_ratio=0.1,              # Keep learning rate constant after warmup\n          logging_steps=50,                        # Log metrics every N steps\n          save_strategy=\"epoch\",                    # Save checkpoint every epoch\n          bf16=True,                                # Use bfloat16 precision\n          push_to_hub=False,                        # Don't push to HuggingFace Hub\n          report_to=\"none\",                         # Disable external logging\n          max_steps=max_steps,\n          dataloader_num_workers=2,# Maximum number of training steps\n      )\n    \n    # Remove the 'dataset_text_field' argument which is not accepted by SFTTrainer\n      trainer = SFTTrainer(\n          model=self.model,\n          train_dataset=self.tokenized_data,\n          args=args,\n          data_collator=DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n      )\n\n      trainer.train()\n\n  \n   \n\n  def save_model(self):\n    \"\"\"\n    this function will save the model\n    \"\"\"\n    self.model.save_pretrained(self.output_dir)\n    self.tokenizer.save_pretrained(self.output_dir)\n    print(f\"Adapter saved to {self.output_dir}\")\n    \n    \n      \n  def run(self):\n    \"\"\"\n    this function will run the whole process\n    \"\"\"\n    print(\"starting finetunine process\")\n    self.load_tokenizer()\n    print(\"tokenizer loaded\")\n\n    self.load_model()\n    print(\"model loaded\")\n\n    self.apply_lora()\n    print(\"lora applied\")\n\n    self.tokenize_dataset()\n    print(\"dataset loaded and tokenized\")\n\n    self.train_model()\n    print(\"model trained\")\n\n    self.save_model()\n    print(\"model saved\")\n","metadata":{"id":"bWJE8kGpCFSZ","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T16:03:30.297816Z","iopub.execute_input":"2025-03-20T16:03:30.298045Z","iopub.status.idle":"2025-03-20T16:03:30.315467Z","shell.execute_reply.started":"2025-03-20T16:03:30.298026Z","shell.execute_reply":"2025-03-20T16:03:30.314855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name=\"mistralai/Mistral-7B-Instruct-v0.1\"\noutput_dir=\"mistreal-7b-finetuned\"\n","metadata":{"id":"48puzPwvF9wN","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T16:03:30.316155Z","iopub.execute_input":"2025-03-20T16:03:30.316347Z","iopub.status.idle":"2025-03-20T16:03:30.336644Z","shell.execute_reply.started":"2025-03-20T16:03:30.316329Z","shell.execute_reply":"2025-03-20T16:03:30.336035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fine_tuner=SFTFineTuner(model_name,data_df,output_dir)","metadata":{"id":"nWQW_dyzhC9w","outputId":"e1d04125-01be-4f96-a68e-1137c5574355","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T16:03:30.337432Z","iopub.execute_input":"2025-03-20T16:03:30.337631Z","iopub.status.idle":"2025-03-20T16:03:30.353112Z","shell.execute_reply.started":"2025-03-20T16:03:30.337605Z","shell.execute_reply":"2025-03-20T16:03:30.352464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"id":"Tm1kQfZ359rQ","outputId":"29488b6d-469f-4c4d-e947-1bb69b0cc67f","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T16:03:30.353864Z","iopub.execute_input":"2025-03-20T16:03:30.354168Z","iopub.status.idle":"2025-03-20T16:03:30.385659Z","shell.execute_reply.started":"2025-03-20T16:03:30.354139Z","shell.execute_reply":"2025-03-20T16:03:30.384797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fine_tuner.run()","metadata":{"id":"hYEXMmJ4GxPt","outputId":"5b2fed5e-fe4c-4b5b-8e11-7a197be7f82a","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T16:03:43.640861Z","iopub.execute_input":"2025-03-20T16:03:43.641213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_finetuned_model(model_name, adapter_path, device):\n    # Load base model with same quantization config as training\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16\n    )\n    \n    base_model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        trust_remote_code=True\n    )\n    \n    # Attach adapter\n    model = PeftModel.from_pretrained(base_model, adapter_path)\n    tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n    \n    return model, tokenizer\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_article(article_text, tokenizer, model, device=\"cuda\",\n                     input_max_length=512, output_max_length=256):\n    # Create prompt matching your training format\n    prompt = f\"\"\"Summarize the following article:\n{article_text}\nSummary:\"\"\"\n    \n    inputs = tokenizer(\n        prompt,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=input_max_length,\n        padding=\"max_length\"\n    ).to(device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=output_max_length,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    # Extract text after \"Summary:\" \n    summary = full_text.split(\"Summary:\")[-1].strip()\n    return summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Usage -------------------------------------------------\nimport time\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\n# Load your saved adapter (assuming output_dir is where you saved)\ntry:\n    model, tokenizer = load_finetuned_model(\n        model_name=\"mistralai/Mistral-7B-Instruct-v0.1\",  # Original model name used in training\n        adapter_path=output_dir,  # self.output_dir from SFTFineTuner\n        device=device\n    )\n    model.eval()\n    \n\n\n    article_text = \"\"\"\nGovernments worldwide are ramping up their climate action efforts ahead of the United Nations Climate\nSummit scheduled for September 2025. With rising global temperatures and frequent extreme weather events, policymakers\nare under increasing pressure to introduce stricter regulations and commit to bolder carbon reduction targets.In Europe,\nthe European Union (EU) announced plans to implement more aggressive emission reduction measures. The EU Commission proposed\na 60% reduction in greenhouse gas emissions by 2035, surpassing the previously set target of 55%. Additionally, they aim to expand\nrenewable energy projects, including solar and wind farms, across member states.Meanwhile, in the United States, the Biden administration\nintroduced new federal guidelines aimed at accelerating the transition to electric vehicles (EVs). The Environmental Protection Agency (EPA)\nproposed regulations that would require automakers to ensure that 67% of new vehicle sales by 2032 are electric. This move is part of the\nadministration's broader goal of achieving net-zero emissions by 2050.\n\"\"\"  # Your article text\n    start_time=time.time()\n\n    generated_summary = evaluate_article(\n        article_text,\n        tokenizer=tokenizer,\n        model=model,\n        device=device\n    )\n    \n    print(f\"Generated article summary in {time.time()-start_time:.2f}s\")\n    print(\"Generated Summary:\\n\", generated_summary)\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n    # Load the CNN/DailyMail dataset again to get reference summaries\n    eval_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")\n    eval_df = pd.DataFrame(eval_dataset)\n\n    # Select a few examples for evaluation (e.g., first 5)\n    num_eval_examples = 5\n    evaluation_articles = eval_df['article'].tolist()[:num_eval_examples]\n    reference_summaries = eval_df['highlights'].tolist()[:num_eval_examples]\n    \n\n    generated_summaries =[]\n    for article_text in evaluation_articles:\n        generated_summary = evaluate_article(\n            article_text,\n            tokenizer=tokenizer,\n            model=model,\n            device=device\n        )\n        generated_summaries.append(generated_summary)\n\n    # Load ROUGE metric\n    rouge = evaluate.load(\"rouge\")\n\n    # Compute ROUGE scores\n    rouge_results = rouge.compute(\n        predictions=generated_summaries,\n        references=reference_summaries,\n        rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"]\n    )\n\n    print(\"\\nROUGE Scores:\")\n    print(f\"ROUGE-1: {rouge_results['rouge1']}\")\n    print(f\"ROUGE-2: {rouge_results['rouge2']}\")\n    print(f\"ROUGE-L: {rouge_results['rougeL']}\")\n\n    # Load BERTScore metric\n    bertscore = evaluate.load(\"bertscore\")\n\n    # Compute BERTScore\n    bertscore_results = bertscore.compute(\n        predictions=generated_summaries,\n        references=reference_summaries,\n        lang=\"en\",\n        model_type=\"microsoft/deberta-xlarge-mnli\"   # You can choose a different model\n    )\n\n    print(\"\\nBERTScore (F1):\")\n    print(f\"Average: {np.mean(bertscore_results['f1']):.4f}\")\n    print(f\"Min: {np.min(bertscore_results['f1']):.4f}\")\n    print(f\"Max: {np.max(bertscore_results['f1']):.4f}\")\n\nexcept Exception as e:\n    print(f\"An error occurred during evaluation: {e}\")\n    print(\"Make sure you have trained and saved the model adapter to the specified output directory.\")\n    print(\"Also, ensure you have the necessary libraries installed (evaluate, bert_score).\")","metadata":{"id":"RbCT-Yd9hX4u","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T16:03:30.610099Z","iopub.status.idle":"2025-03-20T16:03:30.61044Z","shell.execute_reply":"2025-03-20T16:03:30.610303Z"}},"outputs":[],"execution_count":null}]}